# TARSy LLM Provider Configurations
# Copy this file to llm-providers.yaml and customize for your environment
#
# cp llm-providers.yaml.example llm-providers.yaml
#
# IMPORTANT: API keys should be configured via environment variables
# - Define actual API key values in your .env file
# - Use {{.VAR_NAME}} syntax to reference environment variables (Go template syntax)
# - Built-in providers can be overridden by defining same name

# =============================================================================
# GOOGLE / GEMINI PROVIDERS
# =============================================================================

llm_providers:
  # Fast model for quick responses
  gemini-2.5-flash:
    type: google
    model: gemini-2.5-flash
    api_key_env: GOOGLE_API_KEY
    max_tool_result_tokens: 950000  # Conservative for 1M context
    native_tools:
      google_search: true
      code_execution: false
      url_context: true

  # Pro model for complex reasoning
  gemini-2.5-pro:
    type: google
    model: gemini-2.5-pro
    api_key_env: GOOGLE_API_KEY
    max_tool_result_tokens: 950000
    native_tools:
      google_search: true
      code_execution: false
      url_context: true

  # Next-gen model
  gemini-3-pro:
    type: google
    model: gemini-3-pro-preview
    api_key_env: GOOGLE_API_KEY
    max_tool_result_tokens: 950000
    native_tools:
      google_search: true
      code_execution: false
      url_context: true

  # Example: Google via OpenAI-compatible proxy
  openai-gemini-proxy:
    type: openai
    model: gemini-2.5-pro
    api_key_env: OPENAI_API_KEY
    base_url: https://gemini-proxy.example.com/v1beta/openai
    max_tool_result_tokens: 950000

# =============================================================================
# OPENAI PROVIDERS
# =============================================================================

  # o3 for advanced reasoning
  o3:
    type: openai
    model: o3
    api_key_env: OPENAI_API_KEY
    max_tool_result_tokens: 100000

  # o4-mini for fast reasoning
  o4-mini:
    type: openai
    model: o4-mini
    api_key_env: OPENAI_API_KEY
    max_tool_result_tokens: 100000

  # GPT-5 for general use
  gpt-5:
    type: openai
    model: gpt-5
    api_key_env: OPENAI_API_KEY
    max_tool_result_tokens: 100000

# =============================================================================
# ANTHROPIC / CLAUDE PROVIDERS
# =============================================================================

  # Claude Opus for maximum capability
  claude-opus:
    type: anthropic
    model: claude-opus-4-20250514
    api_key_env: ANTHROPIC_API_KEY
    max_tool_result_tokens: 150000  # Conservative for 200K context

  # Claude Sonnet for balanced performance
  claude-sonnet:
    type: anthropic
    model: claude-sonnet-4-20250514
    api_key_env: ANTHROPIC_API_KEY
    max_tool_result_tokens: 150000

  # Claude Haiku for speed
  claude-haiku:
    type: anthropic
    model: claude-haiku-4-20250514
    api_key_env: ANTHROPIC_API_KEY
    max_tool_result_tokens: 150000

# =============================================================================
# X.AI / GROK PROVIDERS
# =============================================================================

  # Grok 4 for X.AI
  grok-4:
    type: xai
    model: grok-4
    api_key_env: XAI_API_KEY
    max_tool_result_tokens: 200000  # Conservative for 256K context

# =============================================================================
# VERTEX AI PROVIDERS (GCP)
# =============================================================================

  # Claude on Vertex AI
  vertexai-claude-sonnet:
    type: vertexai
    model: claude-sonnet-4@20250514
    project_env: GCP_PROJECT
    location_env: GCP_LOCATION
    max_tool_result_tokens: 150000

  # Gemini on Vertex AI
  vertexai-gemini-pro:
    type: vertexai
    model: gemini-2.5-pro
    project_env: GCP_PROJECT
    location_env: GCP_LOCATION
    max_tool_result_tokens: 950000

# =============================================================================
# CUSTOM / ENTERPRISE PROVIDERS
# =============================================================================

  # Example: Custom OpenAI-compatible endpoint
  custom-llm:
    type: openai
    model: custom-model
    api_key_env: CUSTOM_API_KEY
    base_url: https://llm.company.com/v1
    max_tool_result_tokens: 100000

  # Example: Azure OpenAI
  azure-o4-mini:
    type: openai
    model: o4-mini
    api_key_env: AZURE_OPENAI_KEY
    base_url: https://your-resource.openai.azure.com/openai/deployments/your-deployment
    max_tool_result_tokens: 100000
