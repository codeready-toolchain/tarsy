# TARSy PoC Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM Service Configuration (Python)
# =============================================================================

# Required: Google Gemini API Key
GOOGLE_API_KEY=your_google_api_key_here

# Optional: Gemini model to use
GEMINI_MODEL=gemini-2.0-flash-thinking-exp-01-21

# Optional: Temperature for LLM generation (0.0 - 2.0)
GEMINI_TEMPERATURE=1.0

# Optional: Maximum tokens for LLM generation
# GEMINI_MAX_TOKENS=8192

# Optional: gRPC port for LLM service
GRPC_PORT=50051

# =============================================================================
# Go Orchestrator Configuration
# =============================================================================

# gRPC address of LLM service
GRPC_ADDR=localhost:50051

# HTTP server port
HTTP_PORT=8080

# Gin mode: debug, release, test
GIN_MODE=debug
