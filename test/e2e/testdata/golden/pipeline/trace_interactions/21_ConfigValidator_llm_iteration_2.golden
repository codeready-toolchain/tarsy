{
  "created_at": "{TIMESTAMP}",
  "duration_ms": {DURATION_MS},
  "id": "{INTERACTION_ID_19}",
  "input_tokens": 10,
  "interaction_type": "iteration",
  "llm_request": {
    "iteration": 2,
    "messages_count": 4,
    "native_tools": {
      "google_search": true
    }
  },
  "llm_response": {
    "text_length": 74,
    "tool_calls_count": 0
  },
  "model_name": "test-model",
  "output_tokens": 5,
  "thinking_content": "The memory limit of 512Mi matches the alert threshold.",
  "total_tokens": 15
}

=== MESSAGE: system ===
## General SRE Agent Instructions

You are an expert Site Reliability Engineer (SRE) with deep knowledge of:
- Kubernetes and container orchestration
- Cloud infrastructure and services
- Incident response and troubleshooting
- System monitoring and alerting
- GitOps and deployment practices

Analyze alerts thoroughly and provide actionable insights based on:
1. Alert information and context
2. Associated runbook procedures
3. Real-time system data from available tools

Always be specific, reference actual data, and provide clear next steps.
Focus on root cause analysis and sustainable solutions.

## Evidence Transparency

Your conclusions MUST be grounded in evidence you actually gathered, not assumptions:

- **Distinguish data sources**: Clearly separate what you learned from tool results vs. what was already in the alert data. Never present alert metadata as if it were independently verified.
- **Report tool failures honestly**: If a tool call fails, returns empty results, or returns errors â€” say so explicitly. Do not silently proceed as if you have the data.
- **Adjust confidence accordingly**: If most or all tool calls failed, your confidence should be LOW. State that your analysis is based primarily on alert data and lacks independent verification.
- **Flag investigation gaps**: When you could not gather critical data, explicitly state what you were unable to verify and why (e.g., "Tool X returned an error, so I could not confirm Y").
- **Never fabricate evidence**: Do not invent details, metrics, or observations that did not appear in tool results or the alert data.

## Agent-Specific Instructions

You are ConfigValidator, verifying resource configurations.

Focus on investigation and providing recommendations for human operators to execute.

=== MESSAGE: user ===
## Alert Details

### Alert Metadata
**Alert Type:** test-alert

### Alert Data
<!-- ALERT_DATA_START -->
Pod OOMKilled
<!-- ALERT_DATA_END -->

## Runbook Content
````markdown
<!-- RUNBOOK START -->
# Generic Troubleshooting Guide

## Investigation Steps

1. **Analyze the alert** - Review alert data and identify affected system/service
2. **Gather context** - Use tools to check current state and recent changes
3. **Identify root cause** - Investigate potential causes based on alert type
4. **Assess impact** - Determine scope and severity
5. **Recommend actions** - Suggest safe investigation or remediation steps

## Guidelines

- Verify information before suggesting changes
- Consider dependencies and potential side effects
- Document findings and actions taken
- Focus on understanding the problem before proposing solutions
- When in doubt, gather more information rather than making assumptions

## Common Investigation Patterns

### For Performance Issues
- Check resource utilization (CPU, memory, disk, network)
- Review recent deployments or configuration changes
- Analyze metrics and logs for anomalies
- Identify bottlenecks in the request path

### For Availability Issues
- Verify service health and readiness
- Check for recent restarts or crashes
- Review dependencies and upstream services
- Examine load balancer and routing configuration

### For Error Rate Spikes
- Analyze error messages and stack traces
- Correlate with recent deployments
- Check for external service failures
- Review input validation and edge cases

<!-- RUNBOOK END -->
````

## Previous Stage Data
<!-- CHAIN_CONTEXT_START -->

### Stage 1: investigation

Investigation complete: pod-1 is OOMKilled with 5 restarts.

### Stage 2: remediation

Recommend increasing memory limit to 1Gi and adding a HPA for pod-1.

<!-- CHAIN_CONTEXT_END -->

## Your Task
Use the available tools to investigate this alert and provide:
1. Root cause analysis
2. Current system state assessment
3. Specific remediation steps for human operators
4. Prevention recommendations

Be thorough in your investigation before providing the final answer.

For each factual finding about the current state of the system, reference where the data came from (e.g., which tool call, which log entry, which metric). General SRE knowledge does not need citations, but any claim about what is happening in this specific environment must be traceable to a tool result or the alert data.

=== MESSAGE: assistant ===
(tool calls only)
--- TOOL_CALLS ---
[call-c1] test-mcp__get_resource_config({"pod":"pod-1","namespace":"default"})

=== MESSAGE: tool (call-c1, test-mcp__get_resource_config) ===
{"pod":"pod-1","limits":{"memory":"512Mi","cpu":"250m"},"requests":{"memory":"256Mi","cpu":"100m"}}

=== MESSAGE: assistant ===
Config validated: pod-1 memory limit is 512Mi, matching the OOM threshold.

