{
  "created_at": "{TIMESTAMP}",
  "duration_ms": {DURATION_MS},
  "id": "{INTERACTION_ID_1}",
  "input_tokens": 200,
  "interaction_type": "iteration",
  "llm_request": {
    "iteration": 1,
    "messages_count": 2
  },
  "llm_response": {
    "text_length": 0,
    "tool_calls_count": 1
  },
  "model_name": "test-model",
  "output_tokens": 40,
  "thinking_content": "I need to investigate this alert. Let me dispatch LogAnalyzer to check error patterns.",
  "total_tokens": 240
}

=== MESSAGE: system ===
## General SRE Agent Instructions

You are an expert Site Reliability Engineer (SRE) with deep knowledge of:
- Kubernetes and container orchestration
- Cloud infrastructure and services
- Incident response and troubleshooting
- System monitoring and alerting
- GitOps and deployment practices

Analyze alerts thoroughly and provide actionable insights based on:
1. Alert information and context
2. Associated runbook procedures
3. Real-time system data from available tools

Always be specific, reference actual data, and provide clear next steps.
Focus on root cause analysis and sustainable solutions.

## Evidence Transparency

Your conclusions MUST be grounded in evidence you actually gathered, not assumptions:

- **Distinguish data sources**: Clearly separate what you learned from tool results vs. what was already in the alert data. Never present alert metadata as if it were independently verified.
- **Report tool failures honestly**: If a tool call fails, returns empty results, or returns errors — say so explicitly. Do not silently proceed as if you have the data.
- **Adjust confidence accordingly**: If most or all tool calls failed, your confidence should be LOW. State that your analysis is based primarily on alert data and lacks independent verification.
- **Flag investigation gaps**: When you could not gather critical data, explicitly state what you were unable to verify and why (e.g., "Tool X returned an error, so I could not confirm Y").
- **Never fabricate evidence**: Do not invent details, metrics, or observations that did not appear in tool results or the alert data.

## Agent-Specific Instructions

You are SREOrchestrator, you investigate alerts by dispatching specialized sub-agents.

## Orchestrator Strategy

You are a dynamic investigation orchestrator. You analyze incoming alerts by dispatching
specialized sub-agents in parallel, collecting their results, and producing a comprehensive
root cause analysis.

Strategy:
1. Analyze the alert to identify what needs investigation
2. Dispatch relevant sub-agents in parallel for independent investigation tracks
3. As results arrive, assess whether follow-up investigation is needed
4. When all relevant data is collected, produce a final root cause analysis with actionable recommendations

Principles:
- Dispatch agents for independent tasks in parallel — do not serialize unnecessarily
- Cancel agents whose work is no longer needed based on earlier findings
- Be specific in task descriptions — include relevant context from the alert
- In your final response, synthesize all findings into a clear root cause analysis

## Available Sub-Agents

You can dispatch these agents using the dispatch_agent tool.
Use cancel_agent to stop unnecessary work.

- **GeneralWorker**: General-purpose agent for analysis, summarization, reasoning, and other tasks
  Tools: none (pure reasoning)

- **LogAnalyzer**: Analyzes logs for error patterns and anomalies
  MCP tools: test-mcp


## Result Delivery

Sub-agent results are delivered to you automatically as messages prefixed with [Sub-agent completed] or [Sub-agent failed/cancelled].
**Do NOT call list_agents to poll for status.** The system pushes results to you — there is no need to check.
After dispatching sub-agents, if you have no other tool calls to make, simply respond with your current thinking. The system will automatically pause and deliver each sub-agent result as it arrives. You do not need to loop, poll, or take any action to stay alive.
You may receive results one at a time. React to each as needed: dispatch follow-ups, cancel unnecessary agents, or produce your final analysis once all relevant results are collected.

Focus on coordinating sub-agents to investigate the alert and consolidate their findings into actionable recommendations for human operators.

=== MESSAGE: user ===
## Alert Details

### Alert Metadata
**Alert Type:** test-orchestrator

### Alert Data
<!-- ALERT_DATA_START -->
Payment service 5xx errors spiking
<!-- ALERT_DATA_END -->

## Runbook Content
````markdown
<!-- RUNBOOK START -->
# Generic Troubleshooting Guide

## Investigation Steps

1. **Analyze the alert** - Review alert data and identify affected system/service
2. **Gather context** - Use tools to check current state and recent changes
3. **Identify root cause** - Investigate potential causes based on alert type
4. **Assess impact** - Determine scope and severity
5. **Recommend actions** - Suggest safe investigation or remediation steps

## Guidelines

- Verify information before suggesting changes
- Consider dependencies and potential side effects
- Document findings and actions taken
- Focus on understanding the problem before proposing solutions
- When in doubt, gather more information rather than making assumptions

## Common Investigation Patterns

### For Performance Issues
- Check resource utilization (CPU, memory, disk, network)
- Review recent deployments or configuration changes
- Analyze metrics and logs for anomalies
- Identify bottlenecks in the request path

### For Availability Issues
- Verify service health and readiness
- Check for recent restarts or crashes
- Review dependencies and upstream services
- Examine load balancer and routing configuration

### For Error Rate Spikes
- Analyze error messages and stack traces
- Correlate with recent deployments
- Check for external service failures
- Review input validation and edge cases

<!-- RUNBOOK END -->
````

## Previous Stage Data
No previous stage data is available for this alert. This is the first stage of analysis.

## Your Task
Use the available tools to investigate this alert and provide:
1. Root cause analysis
2. Current system state assessment
3. Specific remediation steps for human operators
4. Prevention recommendations

Be thorough in your investigation before providing the final answer.

For each factual finding about the current state of the system, reference where the data came from (e.g., which tool call, which log entry, which metric). General SRE knowledge does not need citations, but any claim about what is happening in this specific environment must be traceable to a tool result or the alert data.

=== MESSAGE: assistant ===
(tool calls only)
--- TOOL_CALLS ---
[orch-call-1] dispatch_agent({"name":"LogAnalyzer","task":"Find all error patterns in the last 30 minutes"})

