# Phase 10: Kubernetes/OpenShift Deployment — Open Questions

Questions where the design departs from old TARSy, involves non-obvious trade-offs, or has multiple valid approaches.

---

## Q1: Pod Topology — Single Pod vs Separate Deployments

**DECIDED: A — Single Pod (4 containers).**

All application containers in one pod, sharing `localhost` network. LLM service reached at `localhost:50051`. One Deployment to manage. Simplest networking, matches podman-compose topology, single atomic rollout. TARSy is single-replica by design so independent scaling is irrelevant. Individual container resource limits within the pod spec address resource isolation.

---

## Q2: API Client Route — External or Internal Only?

**DECIDED: A — Internal only (ClusterIP Service, no Route).**

kube-rbac-proxy is accessible only within the cluster via the `tarsy-api` ClusterIP Service. Only in-cluster ServiceAccounts can authenticate. Simpler setup — no TLS passthrough Route, no separate hostname, SA tokens stay within the cluster. All external access goes through oauth2-proxy as a single entry point. If external programmatic access is needed later, a Route can be added.

---

## Q3: Image Registry Strategy

**DECIDED: A — Dual registry (quay.io + OpenShift internal).**

The OpenShift Kustomize manifests are a dev/test environment that also serves as a reference implementation for production teams. The Dockerfiles and GitHub Actions CI pipeline (building and pushing to quay.io) are the actual production artifacts — production clusters pull from quay.io. Developers use `make openshift-push-*` to push directly to the OpenShift internal registry for fast iteration without waiting for CI. The dev overlay references internal registry images; production deployments (adapted from these manifests) reference quay.io images.

---

## Q4: Overlay Strategy — Development Only or Multiple?

**DECIDED: A — Single development overlay.**

Base contains environment-agnostic structural manifests (Deployments, Services, Routes, RBAC). The `overlays/development/` overlay provides all environment-specific config: namespace (`tarsy`), `configMapGenerator` for config files, and OpenShift internal registry image references. This separation makes the manifests more useful as reference documentation — someone adapting for production can see exactly what needs customizing by looking at the overlay.

---

## Q5: Secrets Management — OpenShift Template vs Alternatives

**DECIDED: A — OpenShift Template.**

Proven pattern from old TARSy: `oc process -f secrets-template.yaml -p KEY=value | oc apply -f -`. Self-documenting (template lists all parameters), auto-generates passwords/cookie secrets. Sufficient for the dev environment. Real production can use ESO (External Secrets Operator) or whatever the production cluster's secret management solution is.

---

## Q6: MCP Server Access in Containers

**DECIDED: B — Remote MCP servers only (HTTP transport).**

In all containerized environments (podman-compose dev, OpenShift dev, real production), MCP servers use `http`/`sse` transport — no stdio MCP servers in containers. The `tarsy.yaml` config overrides the built-in Kubernetes MCP server definition to point to a remote MCP server via HTTP. This keeps the tarsy container clean (no `kubectl` binary, no kubeconfig mounts) and gives MCP servers independent lifecycle. Same approach applies to Phase 9 podman-compose: the mounted `tarsy.yaml` config points MCP servers to remote HTTP endpoints.

---

## Q7: Database Migration Strategy

**DECIDED: Auto-migration on startup (already implemented).**

TARSy already uses **golang-migrate** with versioned `.up.sql` files (not Ent's `Schema.Create()`). Migrations are generated by Atlas CLI from Ent schema diffs and applied automatically on startup via `runMigrations()` → `m.Up()`. This supports any SQL — `ALTER TABLE`, `DROP COLUMN`, `CREATE INDEX`, etc. — not just additive changes. Migrations are forward-only (no `.down.sql`); to undo, create a new forward migration.

**Rolling update safety:** During a rolling update, the new pod applies pending migrations before serving traffic. The old pod continues running against the updated schema. Migration authors must ensure backward compatibility (e.g., add new column first, deploy code that uses it, then drop old column in a later migration). This is standard practice and already works.

---

## Q8: Container Resource Limits

**Context:** Resource requests and limits need to balance container performance against cluster capacity. Old TARSy had: backend 512Mi/1Gi (memory), 500m/1000m (CPU); oauth2-proxy 128Mi/512Mi, 100m/500m; dashboard 128Mi/512Mi, 100m/500m.

**DECIDED: Lower CPU requests for dev environment.**

| Container | Request (mem/cpu) | Limit (mem/cpu) |
|-----------|-------------------|-----------------|
| tarsy | 256Mi / 100m | 1Gi / 1000m |
| llm-service | 256Mi / 100m | 2Gi / 1000m |
| oauth2-proxy | 64Mi / 50m | 512Mi / 500m |
| kube-rbac-proxy | 32Mi / 25m | 128Mi / 200m |

Total pod request: ~608Mi / 275m — lightweight for a dev cluster. Limits stay generous to handle spikes. Production teams can raise requests in their own manifests.

---

## Q9: Namespace Strategy

**DECIDED: Single `tarsy` namespace.**

Only one environment (development) exists in these manifests (see Q3, Q4). The overlay uses `tarsy` as the namespace. If production teams need a different namespace, they adapt the manifests for their environment.

---

## Q10: Serving Certificate for kube-rbac-proxy

**DECIDED: OpenShift service serving certificates (OOTB).**

kube-rbac-proxy needs TLS for its :8443 listener (pod-level TLS, separate from Route-level edge TLS which is fully automatic). One annotation on the `tarsy-api` Service (`service.beta.openshift.io/serving-cert-secret-name`) tells OpenShift to auto-generate and rotate a TLS cert into a Secret. Mount the Secret into the kube-rbac-proxy container — done. The annotation name still says "beta" but it's been the stable, standard mechanism since OpenShift 4.x with no GA rename planned.
