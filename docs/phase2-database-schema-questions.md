# Phase 2: Database Schema - Design Questions

This document contains questions and concerns about the proposed database schema that need discussion before finalizing the design.

**Status**: üü° Pending Discussion  
**Created**: 2026-02-03  
**Purpose**: Identify improvements over old TARSy schema based on lessons learned

---

## How to Use This Document

For each question:
1. ‚úÖ = Decided
2. üîÑ = In Discussion  
3. ‚è∏Ô∏è = Deferred
4. ‚ùå = Rejected

Add your answers inline under each question, then we'll update the main design doc.

---

## üî• Critical Priority (Storage/Performance)

### Q1: Database Schema Architecture - Multi-Layer Model

**Status**: ‚úÖ **DECIDED**

**Original Problems:**
1. Each `LLMInteraction` stores full `conversation` JSON field ‚Üí O(n¬≤) storage growth
2. Frontend receives streamed chunks + DB records ‚Üí Must de-duplicate same content
3. Reasoning tab (UX) and Debug tab (observability) data mixed in same entities
4. Stage vs Agent Execution conflated in single `StageExecution` table

**Decided Solution: Five-Layer Architecture**

---

## Core Hierarchy: Stage + AgentExecution

### Why Two Tables?

**Conceptual Model:**
```
Session
  ‚îú‚îÄ Stage 1: "Initial Analysis" (single agent)
  ‚îÇ   ‚îî‚îÄ AgentExecution 1 (KubernetesAgent)
  ‚îÇ
  ‚îú‚îÄ Stage 2: "Deep Dive" (3 parallel agents)
  ‚îÇ   ‚îú‚îÄ AgentExecution 1 (KubernetesAgent)
  ‚îÇ   ‚îú‚îÄ AgentExecution 2 (ArgoCDAgent)
  ‚îÇ   ‚îî‚îÄ AgentExecution 3 (PrometheusAgent)
  ‚îÇ
  ‚îî‚îÄ Stage 3: "Recommendations" (single agent)
      ‚îî‚îÄ AgentExecution 1 (KubernetesAgent)
```

**Key Insight:** Every stage has 1+ agent executions. No special cases for single vs parallel.

---

### Layer 0a: Stage (Chain Stage - Configuration + Coordination)

**Purpose:** Represents a stage in the processing chain, coordinates agent execution(s)

```
Stage:
// Identity
- stage_id              string    PK, UUID
- session_id            string    FK ‚Üí AlertSession (indexed)

// Stage Configuration
- stage_name            string    "Initial Analysis", "Deep Dive", etc.
- stage_index           int       Position in chain: 0, 1, 2... (indexed)

// Execution Mode
- expected_agent_count  int       How many agents (1 for single, N for parallel)
- parallel_type         *enum     null if count=1, "multi_agent"/"replica" if count>1
- success_policy        *enum     null if count=1, "all"/"any" if count>1

// Stage-Level Status & Timing (aggregated from agent executions)
- status                enum      pending, active, completed, failed, timed_out, cancelled
- started_at            *time.Time  When first agent started
- completed_at          *time.Time  When stage finished (any terminal state)
- duration_ms           *int        Total stage duration
- error_message         *string   Aggregated error if stage failed/timed_out/cancelled

// Chat Context (if applicable)
- chat_id               *string   FK ‚Üí Chat
- chat_user_message_id  *string   FK ‚Üí ChatUserMessage

Indexes:
- (session_id, stage_index) - Unique, stage ordering within session
- stage_id - Primary lookups
```

**Stage Status Aggregation Logic:**

**Key Rule:** Stage remains `active` while ANY agent is `pending` or `active`. Stage status is only determined when ALL agents have terminated.

**Agent Statuses:**
- `pending`: Not yet started (initial state)
- `active`: Currently executing
- `completed`: Finished successfully
- `failed`: Failed with error
- `timed_out`: Exceeded timeout limit
- `cancelled`: Manually cancelled

**Terminal States:** `completed`, `failed`, `timed_out`, `cancelled`

**Aggregation Rules (when all agents terminated):**

**For `success_policy = "all"`** (all agents must succeed):
1. If ALL agents `completed` ‚Üí Stage `completed`
2. Otherwise:
   - If ALL agents `timed_out` ‚Üí Stage `timed_out`
   - If ALL agents `cancelled` ‚Üí Stage `cancelled`
   - Mixed failures ‚Üí We will define the logic of picking the overall session status later.

**For `success_policy = "any"`** (at least one agent must succeed):
1. If ANY agent `completed` ‚Üí Stage `completed` (even if others failed/timed_out/cancelled)
2. Otherwise (all failed):
   - If ALL agents `timed_out` ‚Üí Stage `timed_out`
   - If ALL agents `cancelled` ‚Üí Stage `cancelled`
   - If at least one agent `completed` ‚Üí Stage `completed`
   - Mixed failures (no agent `completed`) ‚Üí We will define the logic of picking the overall session status later.

**Stage stays `active`** while ANY agent is `pending` or `active`

---

### Layer 0b: AgentExecution (Individual Agent Work)

**Purpose:** Each stage has 1+ agent executions. This is where the actual work happens.

```
AgentExecution:
// Identity
- execution_id          string    PK, UUID
- stage_id              string    FK ‚Üí Stage (indexed)
- session_id            string    FK ‚Üí AlertSession (indexed)

// Agent Details
- agent_name            string    "KubernetesAgent", "ArgoCDAgent", etc.
- agent_index           int       1 for single, 1-N for parallel (indexed)

// Execution Status & Timing
- status                enum      pending, active, completed, failed, cancelled, timed_out
- started_at            *time.Time
- completed_at          *time.Time
- duration_ms           *int
- error_message         *string   Error details if failed

// Agent Configuration
- iteration_strategy    string    "react", "native_thinking", etc. (for observability)

Indexes:
- (stage_id, agent_index) - Unique, agent ordering within stage
- execution_id - Primary lookups
- session_id - Session-wide queries
```

**Why both stage_id and session_id?**
- `stage_id`: Required for stage-scoped queries (get all executions for a stage)
- `session_id`: Optimization for session-wide queries (avoid joins)

---

## Context Building Pattern (Lazy Evaluation)

**Design Decision:** No `stage_output` or `agent_output` fields in the database!

### Why No Output Fields?

**Problems with storing output:**
1. ‚ùå **Premature generation**: Stage doesn't know what next stage needs
2. ‚ùå **Wasted computation**: Generated even if no next stage exists
3. ‚ùå **One-size-fits-all**: Can't customize for different consumers

**Solution: Lazy Context Building**

Each agent type implements a `BuildStageContext()` method that:
- Queries its own artifacts (Messages, TimelineEvents, LLMInteractions)
- Formats them appropriately for consumption by next stage
- Called **on-demand** only when next stage actually needs it

### Agent Interface

```go
type Agent interface {
    // Execute the agent
    Execute(ctx context.Context, sessionCtx SessionContext, prevStageContext string) error
    
    // Build context from THIS agent's completed stage
    // Called by next stage when it needs context (lazy evaluation)
    BuildStageContext(ctx context.Context, stageID string) (string, error)
}
```

### Single Agent Example

```go
// KubernetesAgent knows its own structure
func (a *KubernetesAgent) BuildStageContext(ctx context.Context, stageID string) (string, error) {
    // Query this stage's artifacts
    events := a.db.TimelineEvent.Query().
        Where(timelineevent.StageIDEQ(stageID)).
        Order(ent.Asc(timelineevent.FieldSequenceNumber)).
        All(ctx)
    
    messages := a.db.Message.Query().
        Where(message.StageIDEQ(stageID)).
        Order(ent.Asc(message.FieldSequenceNumber)).
        All(ctx)
    
    // Format in KubernetesAgent's own way
    var sb strings.Builder
    sb.WriteString("=== Kubernetes Analysis Results ===\n\n")
    
    // Extract thinking
    for _, event := range events {
        if event.EventType == "llm_thinking" {
            sb.WriteString(fmt.Sprintf("Thinking: %s\n", event.Content))
        }
    }
    
    // Extract tool calls
    for _, event := range events {
        if event.EventType == "mcp_tool_call" {
            sb.WriteString(fmt.Sprintf("Tool %s: %s\n", 
                event.Metadata["tool_name"], event.Content))
        }
    }
    
    // Extract final analysis
    for _, event := range events {
        if event.EventType == "final_analysis" {
            sb.WriteString(fmt.Sprintf("\nConclusion: %s\n", event.Content))
        }
    }
    
    return sb.String(), nil
}
```

### Parallel Agents Example

**When stage has multiple parallel agents, aggregate all their outputs:**

```go
// SynthesisAgent builds context from stage with parallel agents
func (a *SynthesisAgent) BuildStageContext(ctx context.Context, stageID string) (string, error) {
    // Get all agent executions for this stage
    executions := a.db.AgentExecution.Query().
        Where(agentexecution.StageIDEQ(stageID)).
        Order(ent.Asc(agentexecution.FieldAgentIndex)).
        All(ctx)
    
    var sb strings.Builder
    sb.WriteString("=== Synthesis of Parallel Analysis ===\n\n")
    
    // Aggregate context from each agent execution
    for _, exec := range executions {
        sb.WriteString(fmt.Sprintf("--- %s (Agent %d) ---\n", exec.AgentName, exec.AgentIndex))
        
        // Get this agent's timeline events
        events := a.db.TimelineEvent.Query().
            Where(timelineevent.ExecutionIDEQ(exec.ExecutionID)).
            Order(ent.Asc(timelineevent.FieldSequenceNumber)).
            All(ctx)
        
        // Extract final analysis from each agent
        for _, event := range events {
            if event.EventType == "final_analysis" {
                sb.WriteString(fmt.Sprintf("%s\n\n", event.Content))
            }
        }
    }
    
    return sb.String(), nil
}
```

**Key Points for Parallel Agents:**
- Query all `AgentExecution` records for the stage
- Loop through each execution and extract its artifacts
- Aggregate/synthesize into unified context
- Each agent formats its own data appropriately

### Chain Orchestrator Usage

```go
func (c *ChainOrchestrator) ExecuteStage(ctx context.Context, stage StageConfig, prevStageID *string) error {
    var prevContext string
    
    if prevStageID != nil {
        // Lookup which agent type ran previous stage
        prevStage := c.db.Stage.Query().Where(stage.StageIDEQ(*prevStageID)).Only(ctx)
        
        // Get any execution from that stage to determine agent type
        prevExecution := c.db.AgentExecution.Query().
            Where(agentexecution.StageIDEQ(*prevStageID)).
            First(ctx)
        
        // Create agent instance to use its context builder
        prevAgent := c.CreateAgent(prevExecution.AgentName)
        
        // Lazy evaluation - generate context NOW (not at stage completion!)
        prevContext, _ = prevAgent.BuildStageContext(ctx, *prevStageID)
    }
    
    // Execute current stage with context from previous
    agent := c.CreateAgent(stage.AgentName)
    return agent.Execute(ctx, sessionCtx, prevContext)
}
```

### Benefits

‚úÖ **No wasted computation**: Only generate when actually needed  
‚úÖ **No premature decisions**: Next stage specifies what it needs  
‚úÖ **Encapsulation**: Each agent knows its own structure  
‚úÖ **Flexibility**: Can change formatting without schema changes  
‚úÖ **Simpler schema**: No JSON output fields to maintain  
‚úÖ **Works with parallel agents**: Aggregate multiple executions seamlessly

### Future: Optional Caching

If performance becomes a concern, cache generated contexts:

```go
// Optional: Cache formatted context
cacheKey := fmt.Sprintf("stage_context:%s", stageID)
if cached := cache.Get(cacheKey); cached != "" {
    return cached, nil
}
context, _ := agent.BuildStageContext(ctx, stageID)
cache.Set(cacheKey, context, 1*time.Hour)
return context, nil
```

---

### Layer 1: TimelineEvent (Reasoning Tab - UX-focused)

**Purpose:** User-facing investigation timeline, streamed in real-time

```
TimelineEvent:
// Identity & Hierarchy
- event_id            string    PK
- session_id          string    FK ‚Üí AlertSession (indexed)
- stage_id            string    FK ‚Üí Stage (indexed) - Stage grouping
- execution_id        string    FK ‚Üí AgentExecution (indexed) - Which agent

// Timeline Ordering
- sequence_number     int       Order in timeline

// Timestamps
- created_at          time.Time Creation timestamp
- updated_at          time.Time Last update (for streaming)

// Event Details
- event_type          enum      llm_thinking, llm_response, llm_tool_call,
                                mcp_tool_call, mcp_tool_summary,
                                user_question, executive_summary, final_analysis
- status              enum      streaming, completed, failed, cancelled, timed_out
- content             string    Event content (grows during streaming, updateable on completion)
- metadata            JSON      Type-specific data (tool_name, server_name, etc.)

// Debug Links
- llm_interaction_id  *string   Link to debug details (set on completion)
- mcp_interaction_id  *string   Link to debug details (set on completion)

Indexes:
- (session_id, sequence_number) - Timeline ordering
- (stage_id, sequence_number) - Stage timeline grouping
- (execution_id, sequence_number) - Agent timeline filtering
- event_id - Updates by ID
- created_at - Chronological queries
```

**Why both stage_id AND execution_id?**

**Reasoning Tab - Group by stage:**
```go
// Show all events for Stage 2 (all 3 parallel agents combined)
events := client.TimelineEvent.Query().
    Where(timelineevent.StageIDEQ(stageID)).
    Order(ent.Asc(timelineevent.FieldSequenceNumber)).
    All(ctx)
```

**Reasoning Tab - Filter by specific agent in parallel stage:**
```go
// Show only Agent 2's events in Stage 2
events := client.TimelineEvent.Query().
    Where(timelineevent.ExecutionIDEQ(executionID)).
    Order(ent.Asc(timelineevent.FieldSequenceNumber)).
    All(ctx)
```

**Key Features:**
- ‚úÖ **Created IMMEDIATELY** when streaming starts (not after completion)
- ‚úÖ **Updateable** during streaming, immutable after completion
- ‚úÖ **Frontend uses event_id** to track updates - NO de-duplication logic needed!
- ‚úÖ **Single writer** per event (the agent generating it)

**Streaming Flow:**
```
1. Start streaming ‚Üí Create TimelineEvent with status='streaming'
2. During streaming ‚Üí Update content field, stream chunks with event_id
3. Streaming complete ‚Üí Update status='completed'
Frontend: Updates same event_id throughout - no de-duplication! ‚úì
```

---

### Layer 2: Message (LLM Context)

**Purpose:** Conversation history for LLM API calls

```
Message:
// Identity & Hierarchy
- message_id          string    PK
- session_id          string    FK ‚Üí AlertSession (indexed)
- stage_id            string    FK ‚Üí Stage (indexed) - Stage scoping
- execution_id        string    FK ‚Üí AgentExecution (indexed) - Agent conversation

// Message Details
- sequence_number     int       Execution-scoped order
- role                enum      system, user, assistant
- content             string    Message text
- created_at          time.Time Indexed

Indexes:
- (execution_id, sequence_number) - Agent conversation order
- (stage_id, execution_id) - Stage + agent scoping
```

**Why execution_id (not stage_id) for conversation?**

Each agent in a parallel stage has its **own separate conversation**:

```
Stage 2: Deep Dive (3 parallel agents)
  ‚îú‚îÄ AgentExecution 1
  ‚îÇ   ‚îú‚îÄ Message 1: "system: You are KubernetesAgent..."
  ‚îÇ   ‚îú‚îÄ Message 2: "user: Analyze pods..."
  ‚îÇ   ‚îî‚îÄ Message 3: "assistant: I found..."
  ‚îÇ
  ‚îú‚îÄ AgentExecution 2
  ‚îÇ   ‚îú‚îÄ Message 1: "system: You are ArgoCDAgent..."
  ‚îÇ   ‚îú‚îÄ Message 2: "user: Analyze applications..."
  ‚îÇ   ‚îî‚îÄ Message 3: "assistant: I found..."
  ‚îÇ
  ‚îî‚îÄ AgentExecution 3
      ‚îî‚îÄ (separate conversation)
```

**Key Features:**
- ‚úÖ **Execution-scoped**: Each agent has its own conversation
- ‚úÖ **Stage-scoped reset**: Each stage starts with fresh context
- ‚úÖ **Immutable**: Messages never updated once created
- ‚úÖ **Linear storage**: O(n) not O(n¬≤) - no duplication!

**Usage:**
```go
// Get conversation for specific agent execution
messages := service.GetMessagesForExecution(ctx, executionID)

// Build LLM API request
conversation := buildLLMConversation(messages)

// Call LLM
response := llmClient.Call(ctx, conversation)

// Store new assistant message
service.CreateMessage(ctx, Message{
    ExecutionID: executionID,
    Role: "assistant",
    Content: response.Text,
})
```

---

### Layer 3: LLMInteraction (Debug Tab - Observability)

**Purpose:** Full technical details for LLM calls (debugging/analysis)

```
LLMInteraction:
// Identity & Hierarchy
- interaction_id      string    PK
- session_id          string    FK ‚Üí AlertSession (indexed)
- stage_id            string    FK ‚Üí Stage (indexed)
- execution_id        string    FK ‚Üí AgentExecution (indexed) - Which agent

// Timing
- created_at          time.Time Indexed

// Interaction Details
- interaction_type    enum      iteration, final_analysis, executive_summary, chat_response
- model_name          string    "gemini-2.0-flash-thinking-exp", etc.

// Conversation Context (links to Message table)
- REMOVED: conversation field (use Message table instead)
+ last_message_id     *string   FK ‚Üí Message (last message sent to LLM)

// Full API Details
- llm_request         JSON      Full API request payload
- llm_response        JSON      Full API response payload
- thinking_content    *string   Native thinking (Gemini)
- response_metadata   JSON      Grounding, tool usage, etc.

// Metrics & Result
- input_tokens        *int
- output_tokens       *int
- total_tokens        *int
- duration_ms         *int
- error_message       *string   null = success, not-null = failed

Indexes:
- (execution_id, created_at) - Agent's LLM calls chronologically
- (stage_id, created_at) - Stage's LLM calls
- interaction_id - Primary lookups
```

**Conversation Reconstruction:**
```go
// Get the last message that was sent to LLM
lastMessage := client.Message.Get(ctx, interaction.LastMessageID)

// Get all messages up to and including that sequence number
messages := client.Message.Query().
    Where(message.ExecutionIDEQ(interaction.ExecutionID)).
    Where(message.SequenceNumberLTE(lastMessage.SequenceNumber)).
    Order(ent.Asc(message.FieldSequenceNumber)).
    All(ctx)

// These are the exact messages sent as input to this LLM call
```

**Key Features:**
- ‚úÖ **Created on completion** (not during streaming)
- ‚úÖ **Immutable**: Full technical record for audit
- ‚úÖ **Links to Messages**: Conversation reconstructed via `last_message_id`
- ‚úÖ **Full API payloads**: Request/response for debugging
- ‚úÖ **Success/Failure**: Determined by `error_message` (null = success, not-null = failed)

---

### Layer 4: MCPInteraction (Debug Tab - Observability)

**Purpose:** Full technical details for MCP tool calls (debugging/analysis)

```
MCPInteraction:
// Identity & Hierarchy
- interaction_id      string    PK
- session_id          string    FK ‚Üí AlertSession (indexed)
- stage_id            string    FK ‚Üí Stage (indexed)
- execution_id        string    FK ‚Üí AgentExecution (indexed) - Which agent

// Timing
- created_at          time.Time Indexed

// Interaction Details
- interaction_type    enum      tool_call, tool_list
- server_name         string    "kubernetes", "argocd", etc.
- tool_name           *string   "kubectl_get_pods", etc.

// Full Details
- tool_arguments      JSON      Input parameters
- tool_result         JSON      Tool output
- available_tools     JSON      For tool_list type

// Result & Timing
- duration_ms         *int
- error_message       *string   null = success, not-null = failed

Indexes:
- (execution_id, created_at) - Agent's MCP calls chronologically
- (stage_id, created_at) - Stage's MCP calls
- interaction_id - Primary lookups
```

**Key Features:**
- ‚úÖ **Created on completion** (not during streaming)
- ‚úÖ **Immutable**: Full technical record for audit
- ‚úÖ **Full API payloads**: Request/response for debugging
- ‚úÖ **Success/Failure**: Determined by `error_message` (null = success, not-null = failed)

---

## Benefits of Five-Layer Architecture

### ‚úÖ Clean Conceptual Model
```
Stage = Configuration + Coordination + Aggregated Results
AgentExecution = Individual Agent Work
TimelineEvent = UX Timeline
Message = LLM Conversation Context
LLMInteraction/MCPInteraction = Debug Details
```

### ‚úÖ Uniform Stage Model
```
No special "parent execution" entities!
Every stage has 1+ agent executions (single or parallel treated uniformly)
```

### ‚úÖ Solves O(n¬≤) Storage Problem
```
Old: conversation field duplicates all messages in every iteration
     20 iterations = 420 messages stored (should be 40!)
New: Messages stored once in Message table
     20 iterations = 40 messages ‚úì
```

### ‚úÖ Eliminates Frontend De-duplication
```
Old: Stream chunks ‚Üí Store DB record ‚Üí Frontend must de-duplicate ‚úó
New: Create TimelineEvent ‚Üí Stream with event_id ‚Üí Update same event ‚úì
     Frontend just updates existing event by ID!
```

### ‚úÖ Separates Concerns
```
Reasoning Tab ‚Üí Query TimelineEvents (fast, clean UX, stage/agent grouping)
Debug Tab     ‚Üí Query LLMInteraction + MCPInteraction (full technical details)
LLM Context   ‚Üí Query Messages for execution (conversation building)
Chain Logic   ‚Üí Agent.BuildStageContext() generates context on-demand (lazy evaluation)
```

### ‚úÖ Lazy Context Building
```
No stage_output or agent_output in database!
Context generated on-demand when next stage needs it
Each agent knows its own structure and formats appropriately
No wasted computation if no next stage exists
Works seamlessly with parallel agents (aggregate multiple executions)
```

### ‚úÖ Flexible Queries

**Timeline for entire session:**
```go
events := client.TimelineEvent.Query().
    Where(timelineevent.SessionIDEQ(sessionID)).
    Order(ent.Asc(timelineevent.FieldSequenceNumber)).
    All(ctx)
```

**Timeline for a stage (all agents combined):**
```go
events := client.TimelineEvent.Query().
    Where(timelineevent.StageIDEQ(stageID)).
    Order(ent.Asc(timelineevent.FieldSequenceNumber)).
    All(ctx)
```

**Timeline for specific agent in parallel stage:**
```go
events := client.TimelineEvent.Query().
    Where(timelineevent.ExecutionIDEQ(executionID)).
    Order(ent.Asc(timelineevent.FieldSequenceNumber)).
    All(ctx)
```

**Messages for agent's conversation:**
```go
messages := client.Message.Query().
    Where(message.ExecutionIDEQ(executionID)).
    Order(ent.Asc(message.FieldSequenceNumber)).
    All(ctx)
```

**All stages in a session:**
```go
stages := client.Stage.Query().
    Where(stage.SessionIDEQ(sessionID)).
    Order(ent.Asc(stage.FieldStageIndex)).
    All(ctx)
```

**All agent executions for a stage:**
```go
executions := client.AgentExecution.Query().
    Where(agentexecution.StageIDEQ(stageID)).
    Order(ent.Asc(agentexecution.FieldAgentIndex)).
    All(ctx)
```

---

## Implementation Notes

### Event Lifecycle Examples

```
llm_thinking:
  Create with status='streaming' ‚Üí Stream chunks ‚Üí Update status='completed'

mcp_tool_call:
  Create with status='started' ‚Üí Execute tool ‚Üí Update status='completed' + result

user_question (chat):
  Create with status='completed' (instant, no streaming)
```

### Concurrency

- Single writer per event (the generating agent)
- Multiple readers (WebSocket subscribers)
- No optimistic locking needed
- Multiple agents can run concurrently, each writing their own events

### Key Architectural Changes from Old TARSy

**‚úÖ Two-table hierarchy:** Stage (coordination) + AgentExecution (actual work)  
**‚úÖ Lazy context building:** No pre-generated output fields, context built on-demand  
**‚úÖ Parallel agent support:** Uniform model, no special "parent execution" entities  
**‚úÖ No pause feature:** Removed pause/resume complexity (using forced_conclusion instead)  
**‚úÖ Clean separation:** Timeline (UX) / Messages (LLM) / Interactions (Debug) all separate  

---

### Q2: Alert Data Storage & Search

**Status**: ‚úÖ **RESOLVED**

**Context:**
- Alert data is passed as-is to LLM (no parsing or structure required)
- Need ability to search/filter by alert content in dashboard
- No need for structured field extraction (severity, cluster, etc.)

**Decision:**

```
AlertSession:
- alert_data          TEXT      Raw alert string (as received)
```

**Full-Text Search Implementation:**
Use PostgreSQL's built-in full-text search with GIN index:

```sql
-- Ent schema definition:
CREATE INDEX idx_alert_sessions_fts 
ON alert_sessions 
USING GIN(to_tsvector('english', alert_data));

-- Query examples:
-- Simple keyword search:
WHERE to_tsvector('english', alert_data) @@ to_tsquery('error');

-- Boolean operators (AND, OR, NOT):
WHERE to_tsvector('english', alert_data) @@ to_tsquery('error & critical');
WHERE to_tsvector('english', alert_data) @@ to_tsquery('error | warning');
WHERE to_tsvector('english', alert_data) @@ to_tsquery('error & !timeout');

-- Phrase search:
WHERE to_tsvector('english', alert_data) @@ phraseto_tsquery('out of memory');

-- With ranking:
SELECT *, ts_rank(to_tsvector('english', alert_data), to_tsquery('error')) as rank
FROM alert_sessions
WHERE to_tsvector('english', alert_data) @@ to_tsquery('error')
ORDER BY rank DESC;
```

**Benefits:**
- ‚úÖ Very fast even on large datasets (GIN index)
- ‚úÖ Supports stemming (search "running" finds "run", "runs", etc.)
- ‚úÖ Boolean operators (AND, OR, NOT)
- ‚úÖ Relevance ranking
- ‚úÖ No complex parsing or field extraction needed
- ‚úÖ Ent supports GIN indexes natively

**Optional Future Extension:**
If specific structured filtering becomes important later, can add:
```
+ alert_source        *string   Optional: filter by source (prometheus, k8s, custom)
```

---

### Q3: Stage Output Size Management

**Status**: ‚úÖ **RESOLVED** (by Q1 decision)

**Original Problem:**
- `stage_output` JSON stored inline in database
- No size limits or constraints
- Large analysis outputs could bloat database rows
- PostgreSQL 1GB row size limit concern

**Resolution:**
- ‚úÖ **No stage_output or agent_output fields** in the new schema!
- ‚úÖ **Lazy context building** pattern eliminates this concern entirely
- ‚úÖ Context generated on-demand from artifacts (Messages, TimelineEvents)
- ‚úÖ No large JSON blobs stored in Stage or AgentExecution tables

---

## üìã High Priority (Architecture Decisions)

### Q4: Chain Configuration Storage

**Status**: ‚úÖ **RESOLVED**

**Context:**
In old TARSy, `AlertSession` stored both `chain_id` and full `chain_definition` JSON snapshot. The snapshot was used for:
1. **Pause/Resume** - **DROPPED in new TARSy** ‚úÇÔ∏è
2. **Chat configuration** - Check if chat enabled, get agent config, iteration strategy, LLM provider

**Analysis:**

For chat specifically, using a **live lookup** from registry makes more sense than snapshot:
- Chat happens **after** the investigation is complete (not part of immutable investigation record)
- Chat is a separate, optional interaction
- Using latest config means bug fixes and improvements apply to all chats
- No historical consistency requirement (unlike the investigation itself)

**Decision:**

```
AlertSession:
- chain_id            string    Chain identifier (indexed)
```

**No `chain_definition` snapshot stored in database.**

**Rationale:**
- ‚úÖ **No duplication**: 1000 sessions = 1 chain_id string each, not 1000 JSON copies
- ‚úÖ **Always current**: Chat and other features use latest chain configuration
- ‚úÖ **Simpler schema**: One less JSON field to manage
- ‚úÖ **Bug fixes propagate**: Chain config improvements benefit all sessions
- ‚úÖ **Pause/resume dropped**: No need to restore exact historical chain state

**Chain lookup:**
When chat (or other features) needs chain config:
```go
// Look up current chain definition from registry
chainConfig := chainRegistry.GetChain(session.ChainID)
if chainConfig.Chat != nil && !chainConfig.Chat.Enabled {
    return ErrChatDisabled
}
```

**Note:** Chain definitions are stored in code/config files (e.g., `agents.yaml`), loaded at startup into in-memory registry. Not stored in database.

---

### Q5: Integration/Notification Data Modeling

**Status**: ‚úÖ **RESOLVED**

**Context:**
Old TARSy has `slack_message_fingerprint` field directly in `AlertSession` for Slack threading support.

**Decision:**

Keep it simple - **Slack only** for now:

```
AlertSession:
- slack_message_fingerprint  *string  Optional: for Slack message threading
```

**Rationale:**
- ‚úÖ **Simple**: No additional tables or complexity
- ‚úÖ **Sufficient**: Slack is the only notification channel currently needed
- ‚úÖ **Pragmatic**: Avoid premature abstraction
- ‚úÖ **Refactorable**: Easy to extract to separate `Notification` entity later if needed

**Future Extension:**
If additional notification channels (Email, PagerDuty, webhooks) become necessary, refactor to:
```
Notification:
- notification_id     string    PK
- session_id          string    FK ‚Üí AlertSession (indexed)
- notification_type   enum      slack, email, pagerduty, webhook
- integration_data    JSON      Type-specific data (channel, thread_ts, etc.)
- created_at          time.Time
```

For now: **Keep it simple, refactor when needed.**

---

### Q6: Timeline & Debug View Performance

**Status**: ‚úÖ **RESOLVED**

**Context:**
Two different views with different requirements:

1. **Main Session Page** (UX-focused): Uses `TimelineEvent` entities (from Q1)
2. **Debug Page** (Observability): Uses `LLMInteraction` and `MCPInteraction` entities

**Architecture Decision: Separate Pages (Not Tabs)**

Split into two independent pages for better performance and separation of concerns.

---

### Main Session Page: `/sessions/{session_id}`

**What it shows:**
- Session metadata (status, duration, summary)
- Reasoning timeline (TimelineEvents)
- Real-time progress during active session

**API Endpoints:**
```
GET /api/sessions/{id}  ‚Üí Session metadata + TimelineEvents
```

**WebSocket:**
```
/ws/sessions/{id}  ‚Üí TimelineEvent updates (create/update)
```

**Performance:**
- ‚úÖ Single table query (no joins/merging needed)
- ‚úÖ Indexed by `(session_id, sequence_number)`
- ‚úÖ Real-time streaming via WebSocket during active session
- ‚úÖ Fast initial page load (no debug data)
- ‚úÖ Serves 95% of users' needs

---

### Debug Page: `/sessions/{session_id}/debug`

**What it shows:**
- LLM Interactions (collapsed list)
- MCP Interactions (collapsed list)
- Detailed request/response data on expand

**Two-level loading pattern:**

**Level 1: List View (Initial Page Load)**
```sql
-- Just metadata for collapsed view
SELECT 
  interaction_id, 
  interaction_type, 
  created_at,
  model_name,           -- for LLM
  server_name,          -- for MCP
  duration_ms,
  error_message
FROM llm_interactions 
WHERE session_id = ? 
ORDER BY created_at ASC;
```

**Level 2: Detail View (On User Expand)**
```sql
-- Full data when user expands an interaction
SELECT * FROM llm_interactions 
WHERE interaction_id = ?;
```

**API Endpoints:**
```
GET /api/sessions/{id}/debug                      ‚Üí Interaction list (metadata only)
GET /api/sessions/{id}/debug/llm/{interaction_id} ‚Üí Full LLM interaction details
GET /api/sessions/{id}/debug/mcp/{interaction_id} ‚Üí Full MCP interaction details
```

**WebSocket:**
```
/ws/sessions/{id}/debug  ‚Üí Lightweight interaction.created events
```

**WebSocket Event Example:**
```json
{
  "type": "interaction.created",
  "interaction_id": "abc123",
  "interaction_type": "iteration",
  "created_at": "2026-02-03T10:30:00Z"
}
```
- Frontend adds collapsed item to list
- Full interaction data loaded from API when user expands

**Performance:**
- ‚úÖ **List view**: Very fast (no large JSON fields, just metadata)
- ‚úÖ **Detail view**: Lazy loaded only when needed (user expands interaction)
- ‚úÖ **Bandwidth**: Only load full request/response JSON when user wants to see it
- ‚úÖ **Only loaded when needed**: Most users never visit debug page

---

**Database Indexes:**
```sql
-- Main Session Page
CREATE INDEX idx_timeline_events_session ON timeline_events(session_id, sequence_number);

-- Debug Page (list view)
CREATE INDEX idx_llm_interactions_session ON llm_interactions(session_id, created_at);
CREATE INDEX idx_mcp_interactions_session ON mcp_interactions(session_id, created_at);
```

---

**Benefits of Separate Pages:**
- ‚úÖ **Much faster main page load**: Only loads what 95% of users need
- ‚úÖ **Cleaner separation**: Reasoning and Debug are truly independent
- ‚úÖ **Better performance**: Only pay for what you use
- ‚úÖ **Simpler implementation**: No tab state management, clear API endpoints
- ‚úÖ **Better for most users**: Debug data only loaded when explicitly navigated to
- ‚úÖ **Independent WebSocket subscriptions**: Each page subscribes to only what it needs

---

## üìä Medium Priority (Features & Observability)

### Q7: Audit Trail / Change Tracking

**Status**: ‚è∏Ô∏è **DEFERRED**

**Decision:**
Drop audit trail for now. Can implement later if needed.

**Options for future implementation:**
1. **Entity-level auditing**: Track DB changes (before/after snapshots)
2. **API request logging**: Log all API calls (simpler, captures intent + failures)

---

### Q8: LLM Cost Tracking

**Status**: ‚è∏Ô∏è **DEFERRED**

**Decision:**
No cost tracking for now. Store token counts only.

**Current Design:**
```
LLMInteraction:
- input_tokens        *int
- output_tokens       *int
- total_tokens        *int
```

---

### Q9: Event Table Retention & Cleanup

**Status**: ‚úÖ **RESOLVED**

**Context:**
`Event` table used for WebSocket event distribution to live clients during active sessions.

**Decision:**

**Retention:**
- Events only needed for **active sessions**
- Used **only for live updates** (not historical replay)
- No need to retain after session completes

**Cleanup Strategy:**

**Option A: Automatic Cleanup on Session Completion (Ent)**
```go
// When session reaches terminal state (completed, failed, cancelled, timed_out)
func (s *SessionService) CompleteSession(ctx context.Context, sessionID string) error {
    // Update session status
    err := s.client.AlertSession.
        UpdateOneID(sessionID).
        SetStatus(alertsession.StatusCompleted).
        SetCompletedAt(time.Now()).
        Exec(ctx)
    
    if err != nil {
        return err
    }
    
    // Clean up events for this session
    _, err = s.client.Event.
        Delete().
        Where(event.SessionIDEQ(sessionID)).
        Exec(ctx)
    
    return err
}
```

**Option B: TTL-based Cleanup (PostgreSQL + Ent)**
```go
// Add created_at timestamp to Event schema
Event:
+ created_at          time.Time Indexed

// Scheduled cleanup job (e.g., every hour via cron)
func cleanupOldEvents(ctx context.Context, client *ent.Client) error {
    cutoff := time.Now().Add(-24 * time.Hour)
    
    deleted, err := client.Event.
        Delete().
        Where(event.CreatedAtLT(cutoff)).
        Exec(ctx)
    
    log.Printf("Cleaned up %d old events", deleted)
    return err
}
```

**Recommendation: Option A (Session Completion Cleanup)**
- ‚úÖ Simpler: Clean up exactly when no longer needed
- ‚úÖ Efficient: Delete specific session's events
- ‚úÖ Predictable: Events removed immediately on session completion
- ‚úÖ No orphaned events: Handles edge cases (crashes, abandoned sessions)

**Fallback: Add TTL cleanup** as backup to handle edge cases where session completion hook doesn't run.

**Implementation (Ent):**
```go
// Primary: Clean on session completion
func cleanupSessionEvents(ctx context.Context, client *ent.Client, sessionID string) error {
    _, err := client.Event.
        Delete().
        Where(event.SessionIDEQ(sessionID)).
        Exec(ctx)
    return err
}

// Fallback: Periodic cleanup of old events (safety net)
// Cron job runs daily
func cleanupOrphanedEvents(ctx context.Context, client *ent.Client) error {
    cutoff := time.Now().Add(-7 * 24 * time.Hour)
    
    deleted, err := client.Event.
        Delete().
        Where(event.CreatedAtLT(cutoff)).
        Exec(ctx)
    
    log.Printf("Cleaned up %d orphaned events older than 7 days", deleted)
    return err
}
```

**Expected Size:**
- Active sessions at any time: ~10-100
- Events per session: ~50-200
- Total events in table: < 20K rows (very manageable)

---

## üí° Low Priority (Nice-to-Have)

### Q10: Chat Conversation History Storage

**Status**: ‚úÖ **RESOLVED**

**Context:**
Old TARSy stored `conversation_history` in `Chat` table - a formatted snapshot of the investigation for chat context.

**Q1 Impact:**
With **lazy context building** (Q1), we don't pre-generate or store `stage_output` or `agent_output`. Instead, context is built on-demand from artifacts.

**Decision:**

**No `conversation_history` field in `Chat` table.**

**Chat Context Building (On Chat Creation):**
```go
// When user starts a chat, build context on-demand
func (s *ChatService) CreateChat(ctx context.Context, sessionID string) (*Chat, error) {
    // Query session artifacts
    timelineEvents := s.getTimelineEvents(ctx, sessionID)
    messages := s.getMessages(ctx, sessionID)
    
    // Build chat context using ChatAgent's context builder
    // (each agent type knows how to build context from its artifacts)
    chatContext := s.chatAgent.BuildContextForChat(timelineEvents, messages)
    
    // Create chat record (no conversation_history stored)
    chat := &Chat{
        SessionID:  sessionID,
        CreatedBy:  userID,
        // No conversation_history field
    }
    
    return s.client.Chat.Create().SetChat(chat).Save(ctx)
}
```

**Chat Schema (Simplified):**
```
Chat:
- chat_id             string    PK
- session_id          string    FK ‚Üí AlertSession (indexed)
- created_by          string    User who initiated chat
- created_at          time.Time
- mcp_selection       JSON      Optional MCP override
```

**Benefits:**
- ‚úÖ **No duplication**: Don't store data that exists in TimelineEvents/Messages
- ‚úÖ **Always current**: Context built from latest artifacts (if artifacts update, chat sees it)
- ‚úÖ **Consistent with Q1**: Lazy evaluation pattern throughout
- ‚úÖ **Less storage**: One less large TEXT/JSON field per chat

**When chat message sent:**
Context is already in memory from chat creation, or rebuilt from artifacts if needed (e.g., server restart, long-running chat).

---

### Q11: Search & Analytics Support

**Status**: ‚úÖ **RESOLVED**

**Decisions:**

**Q11.1: Full-text search on final analysis**
- **Decision**: Support it (nice to have)
- **Implementation**: Same as Q2 (alert_data)

```
AlertSession:
- final_analysis      TEXT      Investigation summary

-- Add GIN index for full-text search
CREATE INDEX idx_alert_sessions_final_analysis_fts 
ON alert_sessions 
USING GIN(to_tsvector('english', final_analysis));

-- Query examples:
WHERE to_tsvector('english', final_analysis) @@ to_tsquery('memory & leak');
WHERE to_tsvector('english', final_analysis) @@ to_tsquery('error | failure');
```

**Benefits:**
- ‚úÖ Search within investigation summaries
- ‚úÖ Find sessions by analysis keywords
- ‚úÖ Same pattern as alert_data full-text search

---

**Q11.2: Common aggregations**
- **Decision**: Not needed for now
- **Rationale**: Built-in dashboard queries sufficient
- **Future**: Can add materialized views or aggregation tables if performance becomes an issue

---

**Q11.3: BI/Analytics export**
- **Decision**: Out of scope for now
- **Future**: Direct PostgreSQL access or export APIs if needed

---

### Q12: Soft Deletes vs Hard Deletes

**Status**: ‚úÖ **RESOLVED**

**Context:**
- No manual deletion support for now
- Need retention policy for old sessions
- May need to restore soft-deleted sessions if needed

**Decision: Soft Delete for Retention Policy**

**Schema:**
```
AlertSession:
+ deleted_at          *time.Time  Soft delete timestamp (null = active)
```

**Implementation (Ent):**

```go
// Soft delete via retention policy (e.g., sessions older than 90 days)
func softDeleteOldSessions(ctx context.Context, client *ent.Client) error {
    cutoff := time.Now().Add(-90 * 24 * time.Hour)
    
    updated, err := client.AlertSession.
        Update().
        Where(
            alertsession.CompletedAtLT(cutoff),
            alertsession.DeletedAtIsNil(), // Only non-deleted
        ).
        SetDeletedAt(time.Now()).
        Save(ctx)
    
    log.Printf("Soft deleted %d sessions older than 90 days", updated)
    return err
}

// Default queries exclude soft-deleted
func (r *Repository) GetActiveSessions(ctx context.Context) ([]*ent.AlertSession, error) {
    return r.client.AlertSession.
        Query().
        Where(alertsession.DeletedAtIsNil()). // Exclude soft-deleted
        All(ctx)
}

// Restore if needed
func (r *Repository) RestoreSession(ctx context.Context, sessionID string) error {
    return r.client.AlertSession.
        UpdateOneID(sessionID).
        ClearDeletedAt().
        Exec(ctx)
}

// Hard delete (final cleanup, e.g., after 1 year)
func hardDeleteOldSessions(ctx context.Context, client *ent.Client) error {
    cutoff := time.Now().Add(-365 * 24 * time.Hour)
    
    deleted, err := client.AlertSession.
        Delete().
        Where(
            alertsession.DeletedAtNotNil(),      // Only soft-deleted
            alertsession.DeletedAtLT(cutoff),    // Older than 1 year
        ).
        Exec(ctx)
    
    log.Printf("Hard deleted %d soft-deleted sessions older than 1 year", deleted)
    return err
}
```

**Retention Policy Example:**
1. **Day 0-90**: Active sessions (visible in dashboard)
2. **Day 90-365**: Soft-deleted (hidden, but restorable if needed)
3. **Day 365+**: Hard-deleted (permanently removed via CASCADE)

**Benefits:**
- ‚úÖ **Safety net**: Can restore accidentally removed sessions
- ‚úÖ **Gradual cleanup**: Two-phase deletion (soft ‚Üí hard)
- ‚úÖ **Simple queries**: Just add `WHERE deleted_at IS NULL` for active data
- ‚úÖ **Ent support**: Native Ent mixin for soft deletes

**Index:**
```sql
CREATE INDEX idx_alert_sessions_deleted_at ON alert_sessions(deleted_at) 
WHERE deleted_at IS NOT NULL;
```

---

## üìù Summary Checklist

Track which questions we've addressed:

### Critical Priority
- [x] Q1: Database Schema Architecture - Multi-Layer Model (Stage + AgentExecution + TimelineEvent + Message + LLM/MCPInteraction + Lazy Context Building) ‚úÖ **DECIDED**
- [x] Q2: Alert Data Storage & Search (TEXT field + PostgreSQL full-text search with GIN index) ‚úÖ **RESOLVED**
- [x] Q3: Stage Output Size Management ‚úÖ **RESOLVED** (by Q1 - no output fields)

### High Priority
- [x] Q4: Chain Configuration Storage (Just `chain_id`, no snapshot - live lookup from registry) ‚úÖ **RESOLVED**
- [x] Q5: Integration/Notification Modeling (Keep simple: `slack_message_fingerprint` in AlertSession, refactor later if needed) ‚úÖ **RESOLVED**
- [x] Q6: Timeline & Debug View Performance (Reasoning: TimelineEvent query; Debug: 2-level loading with lazy expansion) ‚úÖ **RESOLVED**

### Medium Priority
- [x] Q7: Audit Trail ‚è∏Ô∏è **DEFERRED** (Can implement later if needed)
- [x] Q8: LLM Cost Tracking ‚è∏Ô∏è **DEFERRED** (Token counts stored, no cost calculation for now)
- [x] Q9: Event Retention (Active sessions only, automatic cleanup on completion + TTL fallback) ‚úÖ **RESOLVED**

### Low Priority
- [x] Q10: Chat Conversation History Storage (No storage - build on-demand from artifacts, consistent with Q1 lazy evaluation) ‚úÖ **RESOLVED**
- [x] Q11: Search & Analytics Support (Full-text search on final_analysis, no special aggregations/BI for now) ‚úÖ **RESOLVED**
- [x] Q12: Soft Deletes (Soft delete with `deleted_at` for retention policy, two-phase cleanup) ‚úÖ **RESOLVED**

---

## Next Steps

1. Go through each question in order
2. Add answers inline under each question
3. Mark status (‚úÖ Decided / ‚ùå Rejected / ‚è∏Ô∏è Deferred)
4. Update main design document based on decisions
5. Generate updated Ent schema definitions
